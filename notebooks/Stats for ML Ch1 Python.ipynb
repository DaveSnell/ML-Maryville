{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics for Machine Learning\n",
    "By: Pratap Dangeti\n",
    "Publisher: Packt Publishing\n",
    "Pub. Date: July 21, 2017\n",
    "Web ISBN-13: 978-1-78829-122-4\n",
    "Print ISBN-13: 978-1-78829-575-8\n",
    "Pages in Print Edition: 442       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "\"\"\" First change the following directory link to where all input files do exist \"\"\"\n",
    "os.chdir(\"C:\\dls\\Machine Learning\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 4.33\n",
      "Median : 4.0\n",
      "Mode : 2\n",
      "This took 0.0010013580322265625 seconds to calculate.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "startTime = time.time()\n",
    "data = np.array([4,5,1,2,7,2,6,9,3])\n",
    "\n",
    "# Calculate Mean\n",
    "dt_mean = np.mean(data) ; print (\"Mean :\",round(dt_mean,2))\n",
    "              \n",
    "# Calculate Median                 \n",
    "dt_median = np.median(data) ; print (\"Median :\",dt_median)        \n",
    "\n",
    "# Calculate Mode                     \n",
    "dt_mode =  stats.mode(data); print (\"Mode :\",dt_mode[0][0])  \n",
    "endTime = time.time()\n",
    "print('This took %s seconds to calculate.' % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample variance: 400\n",
      "Sample std.dev: 20.0\n",
      "Range: 69\n",
      "Quantiles:\n",
      "20% 39.8\n",
      "80% 77.4\n",
      "100% 93.0\n",
      "Inter quartile range: 28.5\n",
      "This took 0.003005504608154297 seconds to calculate.\n"
     ]
    }
   ],
   "source": [
    "# Deviance calculations\n",
    "\n",
    "import numpy as np\n",
    "from statistics import variance,stdev\n",
    "\n",
    "startTime = time.time()\n",
    "game_points = np.array([35,56,43,59,63,79,35,41,64,43,93,60,77,24,82])\n",
    "\n",
    "# Calculate Variance\n",
    "dt_var = variance(game_points) ; print (\"Sample variance:\", round(dt_var,2))\n",
    "\n",
    "# Calculate Standard Deviation\n",
    "dt_std = stdev(game_points) ; print (\"Sample std.dev:\",round(dt_std,2))\n",
    "               \n",
    "# Calculate Range\n",
    "dt_rng = np.max(game_points,axis=0) - np.min(game_points,axis=0) ; print (\"Range:\",dt_rng)\n",
    "\n",
    "\n",
    "#Calculate percentiles\n",
    "print (\"Quantiles:\")\n",
    "for val in [20,80,100]:\n",
    "    dt_qntls = np.percentile(game_points,val) \n",
    "    print (str(val)+\"%\" ,dt_qntls)\n",
    "                                \n",
    "# Calculate IQR                           \n",
    "q75, q25 = np.percentile(game_points, [75 ,25]); print (\"Inter quartile range:\",q75-q25 )\n",
    "endTime = time.time()\n",
    "print('This took %s seconds to calculate.' % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Statistic: -4.38\n",
      "Critical value from t-table: -1.699\n",
      "Lower tail p-value from t-table 7.03502572901e-05\n",
      "Prob. to score more than 67 is  17.87 %\n",
      "This took 0.0020728111267089844 seconds to calculate.\n"
     ]
    }
   ],
   "source": [
    "# Hypothesis testing\n",
    "#import scipy                       \n",
    "          \n",
    "from scipy import stats              \n",
    "\n",
    "startTime = time.time()\n",
    "xbar = 990; mu0 = 1000; s = 12.5; n = 30\n",
    "# Test Statistic\n",
    "t_smple  = (xbar-mu0)/(s/np.sqrt(float(n))); print (\"Test Statistic:\",round(t_smple,2))\n",
    "# Critical value from t-table\n",
    "alpha = 0.05\n",
    "t_alpha = stats.t.ppf(alpha,n-1); print (\"Critical value from t-table:\",round(t_alpha,3))          \n",
    "#Lower tail p-value from t-table                        \n",
    "p_val = stats.t.sf(np.abs(t_smple), n-1); print (\"Lower tail p-value from t-table\", p_val)                        \n",
    "                      \n",
    "\n",
    "# Normal Distribution\n",
    "from scipy import stats\n",
    "xbar = 67; mu0 = 52; s = 16.3\n",
    "\n",
    "# Calculating z-score\n",
    "z = (67-52)/16.3\n",
    "\n",
    "# Calculating probability under the curve    \n",
    "p_val = 1- stats.norm.cdf(z)\n",
    "print (\"Prob. to score more than 67 is \",round(p_val*100,2),\"%\")\n",
    "endTime = time.time()\n",
    "print('This took %s seconds to calculate.' % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value is:  0.483\n",
      "This took 0.029573678970336914 seconds to calculate.\n"
     ]
    }
   ],
   "source": [
    "# Chi-square independence test\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "startTime = time.time()\n",
    "survey = pd.read_csv(\"survey.csv\")  \n",
    "# Tabulating 2 variables with row & column variables respectively\n",
    "survey_tab = pd.crosstab(survey.Smoke, survey.Exer, margins = True)\n",
    "# Creating observed table for analysis\n",
    "# book code had ix instead of iloc but it has been deprecated -Dave\n",
    "observed = survey_tab.iloc[0:4,0:3] \n",
    "\n",
    "contg = stats.chi2_contingency(observed= observed)\n",
    "p_value = round(contg[1],3)\n",
    "print (\"P-value is: \",p_value)\n",
    "endTime = time.time()\n",
    "print('This took %s seconds to calculate.' % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic : 3.66 , p-value : 0.051\n",
      "This took 0.001993417739868164 seconds to calculate.\n"
     ]
    }
   ],
   "source": [
    "#ANOVA\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "startTime = time.time()\n",
    "fetilizers = pd.read_csv(\"fetilizers.csv\")\n",
    "\n",
    "one_way_anova = stats.f_oneway(fetilizers[\"fertilizer1\"], fetilizers[\"fertilizer2\"], fetilizers[\"fertilizer3\"])\n",
    "\n",
    "print (\"Statistic :\", round(one_way_anova[0],2),\", p-value :\",round(one_way_anova[1],3))\n",
    "endTime = time.time()\n",
    "print('This took %s seconds to calculate.' % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 0.0030045509338378906 seconds to calculate.\n"
     ]
    }
   ],
   "source": [
    "# Train & Test split\n",
    "import pandas as pd      \n",
    "from sklearn.model_selection import train_test_split              \n",
    "\n",
    "startTime = time.time()\n",
    "original_data = pd.read_csv(\"mtcars.csv\")     \n",
    "\n",
    "train_data,test_data = train_test_split(original_data,train_size = 0.7,random_state=42)\n",
    "endTime = time.time()\n",
    "print('This took %s seconds to calculate.' % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results\n",
      "Intercept 30.0988605396 Coefficient [-0.06822828]\n",
      "This took 0.3032088279724121 seconds to calculate.\n"
     ]
    }
   ],
   "source": [
    "# Linear Regressio vs. Gradient Descent             \n",
    "               \n",
    "import numpy as np                        \n",
    "import pandas as pd\n",
    "                       \n",
    "startTime = time.time()\n",
    "train_data = pd.read_csv(\"mtcars.csv\")                       \n",
    "                        \n",
    "X = np.array(train_data[\"hp\"])  ; y = np.array(train_data[\"mpg\"]) \n",
    "X = X.reshape(32,1); y = y.reshape(32,1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept = True) \n",
    " \n",
    "model.fit(X,y)       \n",
    "print (\"Linear Regression Results\")        \n",
    "print (\"Intercept\",model.intercept_[0] ,\"Coefficient\",model.coef_[0])  \n",
    "endTime = time.time()\n",
    "print('This took %s seconds to calculate.' % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged, iterations:  1143728\n",
      "Gradient Descent Results\n",
      "Intercept = [ 30.02495127] Coefficient = [-0.06781243]\n",
      "This took 715.7425575256348 seconds to calculate.\n"
     ]
    }
   ],
   "source": [
    " def gradient_descent(x, y,learn_rate, conv_threshold,batch_size,max_iter):    \n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = batch_size \n",
    " \n",
    "    t0 = np.random.random(x.shape[1])\n",
    "    t1 = np.random.random(x.shape[1])\n",
    "\n",
    "    MSE = (sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)])/ m)    \n",
    "\n",
    "    while not converged:        \n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)]) \n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "\n",
    "        temp0 = t0 - learn_rate * grad0\n",
    "        temp1 = t1 - learn_rate * grad1\n",
    "    \n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        MSE_New = (sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) / m)\n",
    "\n",
    "        if abs(MSE - MSE_New ) <= conv_threshold:\n",
    "            print ('Converged, iterations: ', iter)\n",
    "            converged = True\n",
    "    \n",
    "        MSE = MSE_New   \n",
    "        iter += 1 \n",
    "    \n",
    "        if iter == max_iter:\n",
    "            print ('Max interactions reached')\n",
    "            converged = True\n",
    "\n",
    "    return t0,t1\n",
    "\n",
    "startTime = time.time()\n",
    "if __name__ == '__main__':\n",
    "    Inter, Coeff = gradient_descent(x = X,y = y,learn_rate=0.00003 ,conv_threshold=1e-8, batch_size=32,max_iter=1500000)\n",
    "    print (\"Gradient Descent Results\")\n",
    "    print (('Intercept = %s Coefficient = %s') %(Inter, Coeff)) \n",
    "    \n",
    "endTime = time.time()\n",
    "print('This took %s seconds to calculate.' % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 0.004014730453491211 seconds to calculate.\n"
     ]
    }
   ],
   "source": [
    "# Train Validation Test split      \n",
    "\n",
    "import pandas as pd      \n",
    "from sklearn.model_selection import train_test_split              \n",
    "                        \n",
    "original_data = pd.read_csv(\"mtcars.csv\")                   \n",
    " \n",
    "\n",
    "def data_split(dat,trf = 0.5,vlf=0.25,tsf = 0.25):\n",
    "    nrows = dat.shape[0]    \n",
    "    trnr = int(nrows*trf)\n",
    "    vlnr = int(nrows*vlf)    \n",
    "    \n",
    "    tr_data,rmng = train_test_split(dat,train_size = trnr,random_state=42)\n",
    "    vl_data, ts_data = train_test_split(rmng,train_size = vlnr,random_state=45)  \n",
    "    \n",
    "    return (tr_data,vl_data,ts_data)\n",
    "\n",
    "startTime = time.time()\n",
    "train_data, validation_data, test_data = data_split(original_data,trf=0.5,vlf=0.25,tsf=0.25)\n",
    "endTime = time.time()\n",
    "print('This took %s seconds to calculate.' % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   30.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best score: \n",
      " 0.967320261438\n",
      "\n",
      " Best parameters set: \n",
      "\n",
      "\tclf__max_depth: 100\n",
      "\tclf__min_samples_leaf: 1\n",
      "\tclf__min_samples_split: 2\n",
      "\n",
      " Confusion Matrix on Test data \n",
      " [[816  17]\n",
      " [ 16 135]]\n",
      "\n",
      " Test Accuracy \n",
      " 0.966463414634\n",
      "\n",
      "Precision Recall f1 table \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98       833\n",
      "          1       0.89      0.89      0.89       151\n",
      "\n",
      "avg / total       0.97      0.97      0.97       984\n",
      "\n",
      "This took 32.37725830078125 seconds to calculate.\n"
     ]
    }
   ],
   "source": [
    "# Grid search on Decision Trees\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "startTime = time.time()\n",
    "input_data = pd.read_csv(\"ad.csv\",header=None)                       \n",
    "\n",
    "X_columns = set(input_data.columns.values)\n",
    "y = input_data[len(input_data.columns.values)-1]\n",
    "X_columns.remove(len(input_data.columns.values)-1)\n",
    "X = input_data[list(X_columns)]\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7,random_state=33)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "parameters = {\n",
    "    'clf__max_depth': (50,100,150),\n",
    "    'clf__min_samples_split': (2, 3),\n",
    "    'clf__min_samples_leaf': (1, 2, 3)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print ('\\n Best score: \\n', grid_search.best_score_)\n",
    "print ('\\n Best parameters set: \\n')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "print (\"\\n Confusion Matrix on Test data \\n\",confusion_matrix(y_test,y_pred))\n",
    "print (\"\\n Test Accuracy \\n\",accuracy_score(y_test,y_pred))\n",
    "print (\"\\nPrecision Recall f1 table \\n\",classification_report(y_test, y_pred))\n",
    "endTime = time.time()\n",
    "print('This took %s seconds to calculate.' % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
