{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression via Gradient Descent in R\n",
    "November 27, 2011\n",
    "By Matt Bogard\n",
    "https://www.r-bloggers.com/regression-via-gradient-descent-in-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  ----------------------------------------------------------------------------------\n",
    "# |PROGRAM NAME: gradient_descent_OLS_R\n",
    "# |DATE: 11/27/11\n",
    "# |CREATED BY: MATT BOGARD \n",
    "# |PROJECT FILE:              \n",
    "# |----------------------------------------------------------------------------------\n",
    "# | PURPOSE: illustration of gradient descent algorithm applied to OLS\n",
    "# | REFERENCE: adapted from : http://www.cs.colostate.edu/~anderson/cs545/Lectures/week6day2/week6day2.pdf                \n",
    "# |  and http://www.statalgo.com/2011/10/17/stanford-ml-1-2-gradient-descent/\n",
    "#  ---------------------------------------------------------------------------------\n",
    "\n",
    "startTime <- Sys.time()\n",
    "# get data \n",
    "rm(list = ls(all = TRUE)) # make sure previous work is clear\n",
    "ls()\n",
    "x0 <- c(1,1,1,1,1) # column of 1's\n",
    "x1 <- c(1,2,3,4,5) # original x-values\n",
    " \n",
    "# create the x- matrix of explanatory variables\n",
    " \n",
    "x <- as.matrix(cbind(x0,x1))\n",
    " \n",
    "# create the y-matrix of dependent variables\n",
    " \n",
    "y <- as.matrix(c(3,7,5,11,14))\n",
    "m <- nrow(y)\n",
    " \n",
    "# implement feature scaling\n",
    "x.scaled <- x\n",
    "x.scaled[,2] <- (x[,2] - mean(x[,2]))/sd(x[,2])\n",
    " \n",
    "# analytical results with matrix algebra\n",
    " solve(t(x)%*%x)%*%t(x)%*%y # w/o feature scaling\n",
    " solve(t(x.scaled)%*%x.scaled)%*%t(x.scaled)%*%y # w/ feature scaling\n",
    " \n",
    "# results using canned lm function match results above\n",
    "summary(lm(y ~ x[, 2])) # w/o feature scaling\n",
    "summary(lm(y ~ x.scaled[, 2])) # w/feature scaling\n",
    " \n",
    "# define the gradient function dJ/dtheata: 1/m * (h(x)-y))*x where h(x) = x*theta\n",
    "# in matrix form this is as follows:\n",
    "grad <- function(x, y, theta) {\n",
    "  gradient <- (1/m)* (t(x) %*% ((x %*% t(theta)) - y))\n",
    "  return(t(gradient))\n",
    "}\n",
    " \n",
    "# define gradient descent update algorithm\n",
    "grad.descent <- function(x, maxit){\n",
    "    theta <- matrix(c(0, 0), nrow=1) # Initialize the parameters\n",
    " \n",
    "    alpha = .05 # set learning rate\n",
    "    for (i in 1:maxit) {\n",
    "      theta <- theta - alpha  * grad(x, y, theta)   \n",
    "    }\n",
    " return(theta)\n",
    "}\n",
    " \n",
    " \n",
    "# results without feature scaling\n",
    "print(grad.descent(x,1000))\n",
    " \n",
    "# results with feature scaling\n",
    "print(grad.descent(x.scaled,1000))\n",
    " \n",
    "# ----------------------------------------------------------------------- \n",
    "# cost and convergence intuition\n",
    "# -----------------------------------------------------------------------\n",
    " \n",
    "# typically we would iterate the algorithm above until the \n",
    "# change in the cost function (as a result of the updated b0 and b1 values)\n",
    "# was extremely small value 'c'. C would be referred to as the set 'convergence'\n",
    "# criteria. If C is not met after a given # of iterations, you can increase the\n",
    "# iterations or change the learning rate 'alpha' to speed up convergence\n",
    " \n",
    "# get results from gradient descent\n",
    "beta <- grad.descent(x,1000)\n",
    " \n",
    "# define the 'hypothesis function'\n",
    "h <- function(xi,b0,b1) {\n",
    " b0 + b1 * xi \n",
    "}\n",
    " \n",
    "# define the cost function   \n",
    "cost <- t(mat.or.vec(1,m))\n",
    "  for(i in 1:m) {\n",
    "    cost[i,1] <-  (1 /(2*m)) * (h(x[i,2],beta[1,1],beta[1,2])- y[i,])^2 \n",
    "  }\n",
    " \n",
    "totalCost <- colSums(cost)\n",
    "print(totalCost)\n",
    " \n",
    "# save this as Cost1000\n",
    "cost1000 <- totalCost\n",
    " \n",
    "# change iterations to 1001 and compute cost1001\n",
    "beta <- (grad.descent(x,1001))\n",
    "cost <- t(mat.or.vec(1,m))\n",
    "  for(i in 1:m) {\n",
    "    cost[i,1] <-  (1 /(2*m)) * (h(x[i,2],beta[1,1],beta[1,2])- y[i,])^2 \n",
    "  }\n",
    "cost1001 <- colSums(cost)\n",
    " \n",
    "# does this difference meet your convergence criteria? \n",
    "print(cost1000 - cost1001)\n",
    "\n",
    "endTime <- Sys.time()\n",
    "timeTaken <- endTime - startTime\n",
    "timeTaken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
